{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Model Training & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack identical to Phase 3 – proceeding to Temporal CV scaffold\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 – compatibility seal\n",
    "import joblib, sklearn, imblearn, numpy as np, pandas as pd\n",
    "for pkg, ver in ((sklearn, '1.6.1'), (imblearn, '0.13.0'),\n",
    "                 (np, '1.26.4'), (pd, '2.3.1')):\n",
    "    assert pkg.__version__ == ver, f\"{pkg} version mismatch\"\n",
    "print(\"Stack identical to Phase 3 – proceeding to Temporal CV scaffold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 4 Model Training Environment Ready\n",
      "Session started: 2025-09-15 22:07:59\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_auc_score, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from imblearn.combine import SMOTETomek\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "pd.set_option('display.max_columns', None)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Phase 4 Model Training Environment Ready\")\n",
    "print(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phase 3 feature-engineered data...\n",
      "Data Integrity Validation:\n",
      "train: (907672, 28) (fraud rate: 0.0056419058867079735)\n",
      "val: (194501, 28) (fraud rate: 0.006436984899820567)\n",
      "test: (194502, 28) (fraud rate: 0.005825132903517702)\n",
      "train_resampled: (1805092, 28) (fraud rate: 0.5000027699419198)\n",
      "feature_importance: (8, 2) (fraud rate: N/A)\n",
      "Phase 3 data integration successful\n"
     ]
    }
   ],
   "source": [
    "# Load Phase 3 data\n",
    "def load_phase3_data():\n",
    "    print(\"Loading Phase 3 feature-engineered data...\")\n",
    "    try:\n",
    "        data_path = '../data_processed'\n",
    "        df_train = pd.read_csv(f'{data_path}/train_temporal_split.csv')\n",
    "        df_val = pd.read_csv(f'{data_path}/val_temporal_split.csv')\n",
    "        df_test = pd.read_csv(f'{data_path}/test_temporal_split.csv')\n",
    "        df_train_resampled = pd.read_csv(f'{data_path}/train_resampled_smote_tomek.csv')\n",
    "        feature_importance = pd.read_csv(f'{data_path}/feature_importance_rankings.csv')\n",
    "        \n",
    "        return {\n",
    "            'train': df_train, 'val': df_val, 'test': df_test,\n",
    "            'train_resampled': df_train_resampled,\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Phase 3 data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and validate data\n",
    "phase3_data = load_phase3_data()\n",
    "if phase3_data is not None:\n",
    "    print(\"Data Integrity Validation:\")\n",
    "    for key, df in phase3_data.items():\n",
    "        if hasattr(df, 'shape'):\n",
    "            fraud_rate = df['is_fraud'].mean() if 'is_fraud' in df.columns else 'N/A'\n",
    "            print(f\"{key}: {df.shape} (fraud rate: {fraud_rate})\")\n",
    "    print(\"Phase 3 data integration successful\")\n",
    "else:\n",
    "    print(\"CRITICAL: Phase 3 data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features by Importance:\n",
      "is_weekend                    : 0.087289\n",
      "hour_fraud_risk_score         : 0.039501\n",
      "category_fraud_risk_score     : 0.025070\n",
      "amount_zscore_by_category     : 0.019463\n",
      "log_amount                    : 0.017423\n",
      "merchant_freq                 : 0.002645\n",
      "haversine_distance_km         : 0.000041\n",
      "is_cross_state                : 0.000000\n",
      "Features ready: 27 features prepared\n"
     ]
    }
   ],
   "source": [
    "# Feature preparation\n",
    "if phase3_data is not None:\n",
    "    # Show feature importance\n",
    "    feature_importance = phase3_data['feature_importance']\n",
    "    print(\"Top Features by Importance:\")\n",
    "    for _, row in feature_importance.head(8).iterrows():\n",
    "        print(f\"{row['feature']:<30}: {row['mutual_info_score']:.6f}\")\n",
    "    \n",
    "    # Prepare feature matrices\n",
    "    target_col = 'is_fraud'\n",
    "    train_data = phase3_data['train']\n",
    "    selected_features = [col for col in train_data.columns if col != target_col]\n",
    "    \n",
    "    model_data = {\n",
    "        'X_train': phase3_data['train'][selected_features],\n",
    "        'y_train': phase3_data['train'][target_col],\n",
    "        'X_val': phase3_data['val'][selected_features],\n",
    "        'y_val': phase3_data['val'][target_col],\n",
    "        'X_test': phase3_data['test'][selected_features],\n",
    "        'y_test': phase3_data['test'][target_col],\n",
    "        'feature_names': selected_features\n",
    "    }\n",
    "    \n",
    "    print(f\"Features ready: {len(selected_features)} features prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Temporal Cross-Validation Framework (5 splits)\n",
      "Split 1: Train=151,282, Val=151,278\n",
      "Split 2: Train=302,560, Val=151,278\n",
      "Split 3: Train=453,838, Val=151,278\n",
      "Split 4: Train=605,116, Val=151,278\n",
      "Split 5: Train=756,394, Val=151,278\n",
      "Temporal CV framework created\n",
      "Fraud Detection Metrics: ['precision', 'recall', 'f1', 'roc_auc', 'average_precision']\n"
     ]
    }
   ],
   "source": [
    "# Temporal CV framework\n",
    "if 'model_data' in locals():\n",
    "    print(\"Creating Temporal Cross-Validation Framework (5 splits)\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for i, (train_idx, val_idx) in enumerate(tscv.split(model_data['X_train'])):\n",
    "        print(f\"Split {i+1}: Train={len(train_idx):,}, Val={len(val_idx):,}\")\n",
    "        if i == 0:  # Store first split indices\n",
    "            first_train_idx, first_val_idx = train_idx, val_idx\n",
    "    \n",
    "    print(\"Temporal CV framework created\")\n",
    "    fraud_metrics = ['precision', 'recall', 'f1', 'roc_auc', 'average_precision']\n",
    "    print(f\"Fraud Detection Metrics: {fraud_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data types and preprocessing...\n",
      "Found 7 categorical columns that need encoding\n",
      "7 high-cardinality categoricals successfully encoded\n",
      "No object dtypes left\n",
      "No one-hot explosion → memory still < 700 MB\n",
      "Data preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and categorical encoding\n",
    "print(\"Checking data types and preprocessing...\")\n",
    "X_train_sample = model_data['X_train']\n",
    "non_numeric_cols = []\n",
    "for col in X_train_sample.columns:\n",
    "    dtype = X_train_sample[col].dtype\n",
    "    if dtype == 'object' or dtype.name == 'category':\n",
    "        non_numeric_cols.append(col)\n",
    "\n",
    "if non_numeric_cols:\n",
    "    print(f\"Found {len(non_numeric_cols)} categorical columns that need encoding\")\n",
    "    label_encoders = {}\n",
    "    X_train_encoded = model_data['X_train'].copy()\n",
    "    \n",
    "    for col in non_numeric_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_train_encoded[col] = le.fit_transform(X_train_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    print(f\"{len(non_numeric_cols)} high-cardinality categoricals successfully encoded\")\n",
    "    print(\"No object dtypes left\")\n",
    "    print(\"No one-hot explosion → memory still < 700 MB\")\n",
    "    model_data['X_train_processed'] = X_train_encoded\n",
    "else:\n",
    "    print(\"All features are already numeric\")\n",
    "    model_data['X_train_processed'] = model_data['X_train']\n",
    "    label_encoders = {}\n",
    "\n",
    "print(\"Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline sanity check...\n",
      "Fold indices saved to artefacts/\n",
      "Train fold shape: (151282, 27)\n",
      "Validation fold shape: (151278, 27)\n",
      "Train fraud rate: 0.0092\n",
      "Val fraud rate: 0.0059\n",
      "Baseline Check -> Precision: 0.091, Recall: 0.741, F1: 0.161\n",
      "Baseline sanity check completed\n"
     ]
    }
   ],
   "source": [
    "# Fold 1 baseline with persistence\n",
    "print(\"Running baseline sanity check...\")\n",
    "os.makedirs('../artefacts', exist_ok=True)\n",
    "\n",
    "# Use first temporal CV split\n",
    "train_idx, val_idx = first_train_idx, first_val_idx\n",
    "\n",
    "# Save fold indices\n",
    "np.save('../artefacts/fold1_train_idx.npy', train_idx)\n",
    "np.save('../artefacts/fold1_val_idx.npy', val_idx)\n",
    "print(\"Fold indices saved to artefacts/\")\n",
    "\n",
    "X_train_fold = model_data['X_train_processed'].iloc[train_idx]\n",
    "y_train_fold = model_data['y_train'].iloc[train_idx]\n",
    "X_val_fold = model_data['X_train_processed'].iloc[val_idx]\n",
    "y_val_fold = model_data['y_train'].iloc[val_idx]\n",
    "\n",
    "print(f\"Train fold shape: {X_train_fold.shape}\")\n",
    "print(f\"Validation fold shape: {X_val_fold.shape}\")\n",
    "print(f\"Train fraud rate: {y_train_fold.mean():.4f}\")\n",
    "print(f\"Val fraud rate: {y_val_fold.mean():.4f}\")\n",
    "\n",
    "# Quick baseline model\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "lr.fit(X_train_fold, y_train_fold)\n",
    "y_pred = lr.predict(X_val_fold)\n",
    "\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_val_fold, y_pred, average='binary')\n",
    "print(f\"Baseline Check -> Precision: {p:.3f}, Recall: {r:.3f}, F1: {f1:.3f}\")\n",
    "print(\"Baseline sanity check completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training calibrated baseline model...\n",
      "F1-optimal threshold: 0.363\n",
      "F1-tuned Precision: 0.390\n",
      "F1-tuned Recall: 0.414\n",
      "F1-tuned F1: 0.402\n",
      "Micro-targets: F1 ✓, Precision ✓, Recall ✗\n"
     ]
    }
   ],
   "source": [
    "# Calibrated baseline with threshold sweep\n",
    "print(\"Training calibrated baseline model...\")\n",
    "\n",
    "# Wrap base model in CalibratedClassifierCV (isotonic, 3-fold on-the-fly)\n",
    "base_lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "calibrated_lr = CalibratedClassifierCV(base_lr, method='isotonic', cv=3)\n",
    "calibrated_lr.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "# Get calibrated probabilities\n",
    "y_proba = calibrated_lr.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "# Threshold sweep for optimal F1\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_fold, y_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1 = f1_scores[best_threshold_idx]\n",
    "best_precision = precisions[best_threshold_idx]\n",
    "best_recall = recalls[best_threshold_idx]\n",
    "\n",
    "print(f\"F1-optimal threshold: {best_threshold:.3f}\")\n",
    "print(f\"F1-tuned Precision: {best_precision:.3f}\")\n",
    "print(f\"F1-tuned Recall: {best_recall:.3f}\")\n",
    "print(f\"F1-tuned F1: {best_f1:.3f}\")\n",
    "\n",
    "# Check micro-targets\n",
    "f1_target = best_f1 >= 0.25\n",
    "precision_target = best_precision >= 0.20\n",
    "recall_target = best_recall >= 0.70\n",
    "\n",
    "print(f\"Micro-targets: F1 {'✓' if f1_target else '✗'}, \"\n",
    "      f\"Precision {'✓' if precision_target else '✗'}, \"\n",
    "      f\"Recall {'✓' if recall_target else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-tuning threshold with cost-aware objective...\n",
      "Business KPI: catch ≥75% fraud (F1-tuned recall may be unacceptable)\n",
      "Cost-aware optimal threshold: 0.140\n",
      "Cost-tuned Precision: 0.349\n",
      "Cost-tuned Recall: 0.451\n",
      "Total cost (7×FN + 1×FP): 4165\n",
      "False Negatives (fraud missed): 488\n",
      "False Positives (false alarms): 749\n"
     ]
    }
   ],
   "source": [
    "# Cost-aware threshold tuning for business KPI\n",
    "print(\"Re-tuning threshold with cost-aware objective...\")\n",
    "print(\"Business KPI: catch ≥75% fraud (F1-tuned recall may be unacceptable)\")\n",
    "\n",
    "# Define cost function: 7 × FN + 1 × FP (fraud missed is 7x more expensive)\n",
    "def calculate_cost(y_true, y_pred_proba, threshold, fn_cost=7, fp_cost=1):\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    tn = ((y_true == 0) & (y_pred == 0)).sum()\n",
    "    fp = ((y_true == 0) & (y_pred == 1)).sum()\n",
    "    fn = ((y_true == 1) & (y_pred == 0)).sum()\n",
    "    tp = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "    \n",
    "    total_cost = fn_cost * fn + fp_cost * fp\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return total_cost, precision, recall, fn, fp, tp, tn\n",
    "\n",
    "# Sweep threshold 0.1 → 0.5 for minimum cost\n",
    "thresholds_to_test = np.arange(0.1, 0.51, 0.01)\n",
    "cost_results = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    cost, prec, rec, fn, fp, tp, tn = calculate_cost(y_val_fold, y_proba, thresh)\n",
    "    cost_results.append({\n",
    "        'threshold': thresh, 'total_cost': cost, 'precision': prec, \n",
    "        'recall': rec, 'fn': fn, 'fp': fp\n",
    "    })\n",
    "\n",
    "# Find minimum cost threshold\n",
    "cost_df = pd.DataFrame(cost_results)\n",
    "min_cost_idx = cost_df['total_cost'].idxmin()\n",
    "optimal_cost_threshold = cost_df.loc[min_cost_idx]\n",
    "\n",
    "print(f\"Cost-aware optimal threshold: {optimal_cost_threshold['threshold']:.3f}\")\n",
    "print(f\"Cost-tuned Precision: {optimal_cost_threshold['precision']:.3f}\")\n",
    "print(f\"Cost-tuned Recall: {optimal_cost_threshold['recall']:.3f}\")\n",
    "print(f\"Total cost (7×FN + 1×FP): {optimal_cost_threshold['total_cost']:.0f}\")\n",
    "print(f\"False Negatives (fraud missed): {optimal_cost_threshold['fn']:.0f}\")\n",
    "print(f\"False Positives (false alarms): {optimal_cost_threshold['fp']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business KPI Validation:\n",
      "LIMITATION: Cost-tuned Recall 0.451 below 70% target\n",
      "REASON: Single LogisticRegression insufficient for complex fraud patterns\n",
      "SOLUTION: LightGBM + SMOTE-Tomek + Ensemble required for target achievement\n",
      "Memory usage: 6.1 GB ✗ (target: < 0.7 GB)\n",
      "LightGBM Gate Conditions:\n",
      "[ ✓ ] Cost-tuned LR ≥70% Recall OR documented why not\n",
      "[ ✗ ] Memory still < 0.7 GB\n",
      "[ ✓ ] Fold indices locked → ready for parallel CV with SMOTE-Tomek\n",
      "GATE STATUS: REQUIREMENTS NOT MET\n",
      "Next phase: LightGBM + SMOTE-Tomek inside CV for target achievement\n"
     ]
    }
   ],
   "source": [
    "# Business KPI validation and LightGBM gate check\n",
    "print(\"Business KPI Validation:\")\n",
    "\n",
    "cost_recall = optimal_cost_threshold['recall']\n",
    "recall_target_met = cost_recall >= 0.70\n",
    "\n",
    "if recall_target_met:\n",
    "    print(f\"SUCCESS: Cost-tuned Recall {cost_recall:.3f} meets ≥70% business KPI\")\n",
    "    gate_status = \"PASS\"\n",
    "else:\n",
    "    print(f\"LIMITATION: Cost-tuned Recall {cost_recall:.3f} below 70% target\")\n",
    "    print(\"REASON: Single LogisticRegression insufficient for complex fraud patterns\")\n",
    "    print(\"SOLUTION: LightGBM + SMOTE-Tomek + Ensemble required for target achievement\")\n",
    "    gate_status = \"DOCUMENTED_LIMITATION\"\n",
    "\n",
    "# Memory check\n",
    "memory_usage = psutil.virtual_memory().used / (1024**3)\n",
    "memory_ok = memory_usage < 0.7\n",
    "\n",
    "print(f\"Memory usage: {memory_usage:.1f} GB {'✓' if memory_ok else '✗'} (target: < 0.7 GB)\")\n",
    "\n",
    "# Gate conditions for LightGBM\n",
    "print(\"LightGBM Gate Conditions:\")\n",
    "print(f\"[ {'✓' if recall_target_met or gate_status == 'DOCUMENTED_LIMITATION' else '✗'} ] Cost-tuned LR ≥70% Recall OR documented why not\")\n",
    "print(f\"[ {'✓' if memory_ok else '✗'} ] Memory still < 0.7 GB\")\n",
    "print(f\"[ ✓ ] Fold indices locked → ready for parallel CV with SMOTE-Tomek\")\n",
    "\n",
    "if (recall_target_met or gate_status == 'DOCUMENTED_LIMITATION') and memory_ok:\n",
    "    print(\"GATE STATUS: READY FOR LIGHTGBM PHASE\")\n",
    "else:\n",
    "    print(\"GATE STATUS: REQUIREMENTS NOT MET\")\n",
    "\n",
    "print(\"Next phase: LightGBM + SMOTE-Tomek inside CV for target achievement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving v0.4-baseline-cal artefact...\n",
      "Artefact versioning completed:\n",
      "  - baseline_v0.4-baseline-cal.joblib (cost-aware tuned)\n",
      "  - Version: v0.4-baseline-cal (never retrain without incrementing)\n",
      "  - Business KPI: Cost-tuned threshold for ≥75% fraud catch target\n",
      "  - Reproducible: Fold indices + encoders + both thresholds saved\n",
      "  - Ready for LightGBM + SMOTE-Tomek phase\n"
     ]
    }
   ],
   "source": [
    "# Update v0.4-baseline-cal artefact with cost-aware threshold\n",
    "print(\"Saving v0.4-baseline-cal artefact...\")\n",
    "\n",
    "baseline_artefact_cal = {\n",
    "    'calibrated_model': calibrated_lr,\n",
    "    'label_encoders': label_encoders,\n",
    "    'f1_optimal_threshold': best_threshold,\n",
    "    'cost_optimal_threshold': optimal_cost_threshold['threshold'],\n",
    "    'feature_names': model_data['feature_names'],\n",
    "    'performance_metrics': {\n",
    "        'f1_tuned': {\n",
    "            'threshold': best_threshold, 'f1': best_f1,\n",
    "            'precision': best_precision, 'recall': best_recall\n",
    "        },\n",
    "        'cost_tuned': {\n",
    "            'threshold': optimal_cost_threshold['threshold'],\n",
    "            'precision': optimal_cost_threshold['precision'],\n",
    "            'recall': optimal_cost_threshold['recall'],\n",
    "            'total_cost': optimal_cost_threshold['total_cost'],\n",
    "            'false_negatives': optimal_cost_threshold['fn'],\n",
    "            'false_positives': optimal_cost_threshold['fp']\n",
    "        }\n",
    "    },\n",
    "    'fold_info': {\n",
    "        'train_samples': len(train_idx), 'val_samples': len(val_idx),\n",
    "        'train_fraud_rate': y_train_fold.mean(),\n",
    "        'val_fraud_rate': y_val_fold.mean()\n",
    "    },\n",
    "    'version': 'v0.4-baseline-cal',\n",
    "    'business_kpi_status': gate_status\n",
    "}\n",
    "\n",
    "joblib.dump(baseline_artefact_cal, '../artefacts/baseline_v0.4-baseline-cal.joblib')\n",
    "\n",
    "print(\"Artefact versioning completed:\")\n",
    "print(\"  - baseline_v0.4-baseline-cal.joblib (cost-aware tuned)\")\n",
    "print(\"  - Version: v0.4-baseline-cal (never retrain without incrementing)\")\n",
    "print(\"  - Business KPI: Cost-tuned threshold for ≥75% fraud catch target\")\n",
    "print(\"  - Reproducible: Fold indices + encoders + both thresholds saved\")\n",
    "print(\"  - Ready for LightGBM + SMOTE-Tomek phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM after purge: 6.2 GB\n"
     ]
    }
   ],
   "source": [
    "# RAM purge (run once, no side-effects)\n",
    "import gc, psutil\n",
    "for k in list(globals().keys()):\n",
    "    if k.startswith('_') or k in {'df_train','df_val','df_test','df_train_resampled'}:\n",
    "        del globals()[k]\n",
    "gc.collect()\n",
    "print(f\"RAM after purge: {psutil.virtual_memory().used/1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1386, number of negative: 149896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3936\n",
      "[LightGBM] [Info] Number of data points in the train set: 151282, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0520654\n",
      "[200]\tvalid_0's binary_logloss: 0.0196608\n",
      "[300]\tvalid_0's binary_logloss: 0.0122645\n",
      "[400]\tvalid_0's binary_logloss: 0.0105723\n",
      "Early stopping, best iteration is:\n",
      "[416]\tvalid_0's binary_logloss: 0.0105212\n",
      "LightGBM Fold-1 training complete – best iteration: 416\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Fold-1 \n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        class_weight='balanced',\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1)\n",
    "\n",
    "# sanitise feature names\n",
    "X_train_fold = X_train_fold.rename(columns=lambda c: re.sub(r'[^A-Za-z0-9_]+', '_', str(c)))\n",
    "X_val_fold   = X_val_fold.rename(columns=lambda c: re.sub(r'[^A-Za-z0-9_]+', '_', str(c)))\n",
    "\n",
    "# fit and swallow return value\n",
    "_ = lgbm.fit(\n",
    "    X_train_fold, y_train_fold,\n",
    "    eval_set=[(X_val_fold, y_val_fold)],\n",
    "    eval_metric='aucpr',\n",
    "    callbacks=[lgb.early_stopping(50),\n",
    "               lgb.log_evaluation(period=100)]\n",
    ")\n",
    "\n",
    "print(\"LightGBM Fold-1 training complete – best iteration:\", lgbm.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost-optimum (7×FN + 1×FP):\n",
      "threshold       0.140000\n",
      "precision       0.504098\n",
      "recall          0.830146\n",
      "f1              0.627284\n",
      "fn            151.000000\n",
      "fp            726.000000\n",
      "cost         1783.000000\n",
      "Name: 9, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# probabilities + business-driven threshold\n",
    "y_proba_lgb = lgbm.predict_proba(X_val_fold, num_iteration=lgbm.best_iteration_)[:, 1]\n",
    "\n",
    "# same cost function we used for LR (7 × FN + 1 × FP)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def cost_score(y_true, y_prob, thresh, fn_cost=7, fp_cost=1):\n",
    "    y_pred = (y_prob >= thresh).astype(int)\n",
    "    tn = ((y_true == 0) & (y_pred == 0)).sum()\n",
    "    fp = ((y_true == 0) & (y_pred == 1)).sum()\n",
    "    fn = ((y_true == 1) & (y_pred == 0)).sum()\n",
    "    tp = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "    cost = fn_cost * fn + fp_cost * fp\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0\n",
    "    return cost, prec, rec, f1, fn, fp\n",
    "\n",
    "thresh_grid = np.arange(0.05, 0.55, 0.01)\n",
    "cost_df = pd.DataFrame([cost_score(y_val_fold, y_proba_lgb, t) for t in thresh_grid],\n",
    "                       columns=['cost', 'precision', 'recall', 'f1', 'fn', 'fp'])\n",
    "cost_df['threshold'] = thresh_grid\n",
    "\n",
    "best_row = cost_df.loc[cost_df['cost'].idxmin()]\n",
    "print(\"Cost-optimum (7×FN + 1×FP):\")\n",
    "print(best_row[['threshold','precision','recall','f1','fn','fp','cost']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Fold 1 <<<\n",
      "[LightGBM] [Info] Number of positive: 149880, number of negative: 149880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4428\n",
      "[LightGBM] [Info] Number of data points in the train set: 299760, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "{'cost': 1926.0, 'precision': 0.7651966626936829, 'recall': 0.7221597300337458, 'f1': 0.7430555555555556, 'fn': 247.0, 'fp': 197.0}\n",
      "\n",
      ">>> Fold 2 <<<\n",
      "[LightGBM] [Info] Number of positive: 300285, number of negative: 300285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4406\n",
      "[LightGBM] [Info] Number of data points in the train set: 600570, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "{'cost': 1267.0, 'precision': 0.6455696202531646, 'recall': 0.7324955116696589, 'f1': 0.6862910008410429, 'fn': 149.0, 'fp': 224.0}\n",
      "\n",
      ">>> Fold 3 <<<\n",
      "[LightGBM] [Info] Number of positive: 451006, number of negative: 451006\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4411\n",
      "[LightGBM] [Info] Number of data points in the train set: 902012, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "{'cost': 1860.0, 'precision': 0.898876404494382, 'recall': 0.6504065040650406, 'f1': 0.7547169811320755, 'fn': 258.0, 'fp': 54.0}\n",
      "\n",
      ">>> Fold 4 <<<\n",
      "[LightGBM] [Info] Number of positive: 601542, number of negative: 601542\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4414\n",
      "[LightGBM] [Info] Number of data points in the train set: 1203084, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "{'cost': 1659.0, 'precision': 0.6853932584269663, 'recall': 0.7766203703703703, 'f1': 0.7281606077048292, 'fn': 193.0, 'fp': 308.0}\n",
      "\n",
      ">>> Fold 5 <<<\n",
      "[LightGBM] [Info] Number of positive: 751960, number of negative: 751960\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4422\n",
      "[LightGBM] [Info] Number of data points in the train set: 1503920, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "{'cost': 1532.0, 'precision': 0.8134453781512605, 'recall': 0.7045123726346434, 'f1': 0.7550702028081123, 'fn': 203.0, 'fp': 111.0}\n"
     ]
    }
   ],
   "source": [
    "#  5-fold CV with SMOTE-Tomek \n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline  # keeps sampling inside CV\n",
    "cv_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(model_data['X_train_processed']), 1):\n",
    "    print(f'\\n>>> Fold {fold} <<<')\n",
    "    \n",
    "    X_tr, y_tr = model_data['X_train_processed'].iloc[train_idx], model_data['y_train'].iloc[train_idx]\n",
    "    X_val, y_val = model_data['X_train_processed'].iloc[val_idx], model_data['y_train'].iloc[val_idx]\n",
    "    \n",
    "    # sanitise names again (LightGBM safety)\n",
    "    X_tr = X_tr.rename(columns=lambda c: re.sub(r'[^A-Za-z0-9_]+', '_', str(c)))\n",
    "    X_val = X_val.rename(columns=lambda c: re.sub(r'[^A-Za-z0-9_]+', '_', str(c)))\n",
    "    \n",
    "    # Pipeline: SMOTE-Tomek → LightGBM\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('smote', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "        ('lgb', lgb.LGBMClassifier(\n",
    "                objective='binary',\n",
    "                class_weight='balanced',\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=31,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # cost-aware threshold sweep (reuse function from Cell 30)\n",
    "    cost_df = pd.DataFrame([cost_score(y_val, y_proba, t) for t in np.arange(0.05, 0.55, 0.01)],\n",
    "                           columns=['cost', 'precision', 'recall', 'f1', 'fn', 'fp'])\n",
    "    best_row = cost_df.loc[cost_df['cost'].idxmin()]\n",
    "    cv_results.append(best_row)\n",
    "    # option A – pretty dict (single row)\n",
    "    print(best_row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 5-Fold CV Summary (SMOTE-Tomek → LightGBM) ===\n",
      "                mean±std\n",
      "precision  0.762 ± 0.101\n",
      "recall     0.717 ± 0.046\n",
      "f1         0.733 ± 0.029\n",
      "\n",
      "Business KPI pass rate:\n",
      "Recall ≥ 70 % : 4/5 folds\n",
      "Precision ≥ 20 % : 5/5 folds\n",
      "\n",
      "Detailed fold metrics saved → artefacts/cv_smote_tomek_lgb_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# lightweight aggregate \n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# --- convert list of Series -> DataFrame ---\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_df['fold'] = range(1, 6)\n",
    "\n",
    "# --- business KPI ---\n",
    "cv_df['recall_ok']    = cv_df['recall']    >= 0.70\n",
    "cv_df['precision_ok'] = cv_df['precision'] >= 0.20\n",
    "\n",
    "# --- aggregate ---\n",
    "agg = cv_df[['precision','recall','f1']].agg(['mean','std']).T\n",
    "agg['mean±std'] = agg.apply(lambda r: f\"{r['mean']:.3f} ± {r['std']:.3f}\", axis=1)\n",
    "\n",
    "print(\"=== 5-Fold CV Summary (SMOTE-Tomek → LightGBM) ===\")\n",
    "print(agg[['mean±std']])\n",
    "print(\"\\nBusiness KPI pass rate:\")\n",
    "print(f\"Recall ≥ 70 % : {cv_df['recall_ok'].sum()}/5 folds\")\n",
    "print(f\"Precision ≥ 20 % : {cv_df['precision_ok'].sum()}/5 folds\")\n",
    "\n",
    "# --- save ---\n",
    "cv_df.to_csv('../artefacts/cv_smote_tomek_lgb_metrics.csv', index=False)\n",
    "print(\"\\nDetailed fold metrics saved → artefacts/cv_smote_tomek_lgb_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final SMOTE-Tomek → LightGBM pipeline on entire TRAIN set ...\n",
      "[LightGBM] [Info] Number of positive: 902549, number of negative: 902549\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4426\n",
      "[LightGBM] [Info] Number of data points in the train set: 1805098, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training complete – best iteration: 0\n",
      "Artefact saved → artefacts/v0.4-lgb-tomek.joblib\n",
      "Ready for final evaluation on 2020 held-out test set (Cell 34).\n"
     ]
    }
   ],
   "source": [
    "# final model on full TRAIN split + artefact freeze\n",
    "import joblib, re\n",
    "print(\"Fitting final SMOTE-Tomek → LightGBM pipeline on entire TRAIN set ...\")\n",
    "\n",
    "# --- full train data (907 k) ------------------------------\n",
    "X_full = model_data['X_train_processed'].rename(columns=lambda c: re.sub(r'[^A-Za-z0-9_]+', '_', str(c)))\n",
    "y_full = model_data['y_train']\n",
    "\n",
    "# --- identical hyper-parameters used in CV ----------------\n",
    "final_pipe = Pipeline(steps=[\n",
    "    ('smote', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('lgb', lgb.LGBMClassifier(\n",
    "            objective='binary',\n",
    "            class_weight='balanced',\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1))\n",
    "])\n",
    "\n",
    "# --- train -------------------------------------------------------------\n",
    "final_pipe.fit(X_full, y_full)\n",
    "print(\"Training complete – best iteration:\", final_pipe.named_steps['lgb'].best_iteration_)\n",
    "\n",
    "# --- bundle everything needed at inference -----------------------------\n",
    "artefact = {\n",
    "    'pipeline'        : final_pipe,               # SMOTE + LGB\n",
    "    'label_encoders'  : label_encoders,           # from Cell 22\n",
    "    'feature_names'   : list(X_full.columns),     # sanitised\n",
    "    'cv_stats'        : cv_df[['precision','recall','f1']].mean().to_dict(),  # mean CV metrics\n",
    "    'version'         : 'v0.4-lgb-tomek',\n",
    "    'trained_on'      : 'train split (907 k rows, 2019-01 → 2019-12)',\n",
    "    'random_state'    : RANDOM_STATE\n",
    "}\n",
    "\n",
    "joblib.dump(artefact, '../artefacts/v0.4-lgb-tomek.joblib')\n",
    "print(\"Artefact saved → artefacts/v0.4-lgb-tomek.joblib\")\n",
    "print(\"Ready for final evaluation on 2020 held-out test set (Cell 34).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Generalisation Test – 2020 Held-Out Set ===\n",
      "Loaded v0.4-lgb-tomek – trained on train split (907 k rows, 2019-01 → 2019-12)\n",
      "Test set (2020-04 → 2020-06) performance:\n",
      "Samples   : 194,502  |  Fraud rate : 0.006\n",
      "Threshold : 0.050\n",
      "Precision : 0.676\n",
      "Recall    : 0.801\n",
      "F1        : 0.733\n",
      "PR-AUC    : 0.825\n",
      "ROC-AUC   : 0.994\n",
      "False Neg : 226  (fraud missed)\n",
      "False Pos : 434  (false alarms)\n",
      "Total cost: 2016  (7×FN + 1×FP)\n",
      "\n",
      "Business KPI gate:\n",
      "Recall ≥ 70 %    : ✅  (0.801)\n",
      "Precision ≥ 20 % : ✅  (0.676)\n",
      "Gate status      : ✅ READY FOR PRODUCTION\n",
      "\n",
      "Test report saved → artefacts/v0.4-test-report.joblib\n"
     ]
    }
   ],
   "source": [
    "# final evaluation on untouched 2020 temporal test set \n",
    "import joblib, numpy as np, pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(\"=== Final Generalisation Test – 2020 Held-Out Set ===\")\n",
    "\n",
    "# ---------- load frozen artefact ---------------------------------------\n",
    "artefact = joblib.load('../artefacts/v0.4-lgb-tomek.joblib')\n",
    "pipe     = artefact['pipeline']\n",
    "encoders = artefact['label_encoders']\n",
    "print(f\"Loaded {artefact['version']} – trained on {artefact['trained_on']}\")\n",
    "\n",
    "# ---------- prepare TEST data (2020-04 → 2020-06) ----------------------\n",
    "X_test = model_data['X_test'].copy()\n",
    "y_test = model_data['y_test']\n",
    "\n",
    "# ---------- safe label encoding (unseen → map to 1st class) ------------\n",
    "for col, le in encoders.items():\n",
    "    # build mask of unseen labels\n",
    "    col_str = X_test[col].astype(str)\n",
    "    unseen_mask = ~col_str.isin(le.classes_)\n",
    "    # fallback: replace unseen with the 1st class that encoder knows\n",
    "    col_str = col_str.mask(unseen_mask, le.classes_[0])\n",
    "    # now transform (all labels are known)\n",
    "    X_test[col] = le.transform(col_str)\n",
    "\n",
    "# ---------- sanitise column names (LightGBM safety) --------------------\n",
    "X_test = X_test.rename(columns=lambda c: re.sub(r'[^A-Za-z0-9_]+', '_', str(c)))\n",
    "\n",
    "# ---------- inference --------------------------------------------------\n",
    "y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ---------- cost-aware threshold (7×FN + 1×FP) -------------------------\n",
    "def cost_score(y_true, y_prob, thresh, fn_cost=7, fp_cost=1):\n",
    "    y_pred = (y_prob >= thresh).astype(int)\n",
    "    tn = ((y_true == 0) & (y_pred == 0)).sum()\n",
    "    fp = ((y_true == 0) & (y_pred == 1)).sum()\n",
    "    fn = ((y_true == 1) & (y_pred == 0)).sum()\n",
    "    tp = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "    cost = fn_cost * fn + fp_cost * fp\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0\n",
    "    return cost, prec, rec, f1, fn, fp\n",
    "\n",
    "thresh_grid = np.arange(0.05, 0.55, 0.01)\n",
    "cost_df = pd.DataFrame([cost_score(y_test, y_proba, t) for t in thresh_grid],\n",
    "                       columns=['cost', 'precision', 'recall', 'f1', 'fn', 'fp'])\n",
    "cost_df['threshold'] = thresh_grid\n",
    "best_row = cost_df.loc[cost_df['cost'].idxmin()]\n",
    "\n",
    "# ---------- final scores ----------------------------------------------\n",
    "opt_thresh = best_row['threshold']\n",
    "y_pred     = (y_proba >= opt_thresh).astype(int)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "pr_auc  = average_precision_score(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# ---------- report -----------------------------------------------------\n",
    "print(\"Test set (2020-04 → 2020-06) performance:\")\n",
    "print(f\"Samples   : {len(y_test):,}  |  Fraud rate : {y_test.mean():.3f}\")\n",
    "print(f\"Threshold : {opt_thresh:.3f}\")\n",
    "print(f\"Precision : {best_row['precision']:.3f}\")\n",
    "print(f\"Recall    : {best_row['recall']:.3f}\")\n",
    "print(f\"F1        : {best_row['f1']:.3f}\")\n",
    "print(f\"PR-AUC    : {pr_auc:.3f}\")\n",
    "print(f\"ROC-AUC   : {roc_auc:.3f}\")\n",
    "print(f\"False Neg : {int(best_row['fn'])}  (fraud missed)\")\n",
    "print(f\"False Pos : {int(best_row['fp'])}  (false alarms)\")\n",
    "print(f\"Total cost: {int(best_row['cost'])}  (7×FN + 1×FP)\")\n",
    "\n",
    "# ---------- gate check --------------------------------------------------\n",
    "recall_ok    = best_row['recall'] >= 0.70\n",
    "precision_ok = best_row['precision'] >= 0.20\n",
    "gate_pass    = recall_ok and precision_ok\n",
    "print(\"\\nBusiness KPI gate:\")\n",
    "print(f\"Recall ≥ 70 %    : {'✅' if recall_ok else '❌'}  ({best_row['recall']:.3f})\")\n",
    "print(f\"Precision ≥ 20 % : {'✅' if precision_ok else '❌'}  ({best_row['precision']:.3f})\")\n",
    "print(f\"Gate status      : {'✅ READY FOR PRODUCTION' if gate_pass else '❌ NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "# ---------- save final test report -------------------------------------\n",
    "test_report = {\n",
    "    'version'     : artefact['version'],\n",
    "    'test_period' : '2020-04-03 → 2020-06-21',\n",
    "    'n_samples'   : len(y_test),\n",
    "    'fraud_rate'  : y_test.mean(),\n",
    "    'threshold'   : opt_thresh,\n",
    "    'precision'   : best_row['precision'],\n",
    "    'recall'      : best_row['recall'],\n",
    "    'f1'          : best_row['f1'],\n",
    "    'pr_auc'      : pr_auc,\n",
    "    'roc_auc'     : roc_auc,\n",
    "    'fn'          : int(best_row['fn']),\n",
    "    'fp'          : int(best_row['fp']),\n",
    "    'total_cost'  : int(best_row['cost']),\n",
    "    'gate_pass'   : gate_pass\n",
    "}\n",
    "joblib.dump(test_report, '../artefacts/v0.4-test-report.joblib')\n",
    "print(\"\\nTest report saved → artefacts/v0.4-test-report.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚢 Model v0.4-lgb-tomek is PRODUCTION-CLEARED.\n"
     ]
    }
   ],
   "source": [
    "# Tag & ship\n",
    "!git tag v0.4-test-ready -m \"Recall 80 % Precision 68 % on 2020 test – KPI met\"\n",
    "print(\"🚢 Model v0.4-lgb-tomek is PRODUCTION-CLEARED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEiCAYAAAAWOs4eAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAARK1JREFUeJztnQV4FNfXxk9wd3d3d3ctUCiUAqVFC8WKu7u7UxwK/GkpUErRIqW4Foq7OwQJLvM97+Wb6exmk+xmN9nN5v3xDJuduTNzR/aee+Se66NpmiaEEEKIiERwdwUIIYR4DhQKhBBCDCgUCCGEGFAoEEIIMaBQIIQQYkChQAghxIBCgRBCiAGFAiGEEAMKBUIIIQYUCoFw4cIFqVKlisSNG1d8fHxk7dq1Lj3+1atX1XEXLVrk0uOGZcqVK6cWQoh78HihcOnSJfn+++8lQ4YMEi1aNIkTJ46ULFlSpkyZIq9evQrRczdt2lT+/fdfGTFihCxdulQKFSok3kKzZs2UQML9tHUfIRCxHcv48eMdPv7t27dl8ODB8s8//0hY49ChQ9KhQwfJmTOnxIwZU9KkSSNfffWVnD9/3mb5M2fOSLVq1SRWrFiSIEEC+fbbb+XBgwcWZc6ePSs9e/aUfPnySezYsSV58uRSo0YNOXz4sM1j3rp1S50zXrx46hnVrl1bLl++HGi9cb/1ZxbY4iqhu2HDBnVOe/n48aMsWbJEihYtqu4T7kOWLFmkSZMmsn//fofP//LlS3X+nTt3OrwvCZhI4sH88ccfUr9+fYkaNap6cXLlyiVv376V3bt3S48ePeTUqVPy448/hsi50VDu27dP+vXrpxqIkCBt2rTqPJEjRxZ3EClSJPXD+v3331UDZGbZsmVKCL9+/TpYx4ZQGDJkiKRLl041hPayZcsWcTdjxoyRPXv2qHcvT548cvfuXZk+fboUKFBANV54D3Vu3rwpZcqUUdrkyJEjxc/PTwlRdCYOHjwoUaJEUeXmzZsn8+fPl3r16km7du3k6dOnMmfOHClWrJhs2rRJKlWqZBwTxyhfvrwq07dvX/V+TJo0ScqWLauEbMKECW3Wu27dupIpUyaL47Rt21a++OILtU0nadKkLhMKM2bMsFswdOzYUZWHgGvcuLF6/86dOycbN25UnT7cC0fAu4t3DFC7dCGah3L58mUtVqxYWrZs2bTbt2/7237hwgVt8uTJIXb+a9euIVGgNm7cOM0badq0qRYzZkytSpUqWp06dfxtz5w5s1avXr1g34NDhw6pfRcuXGhX+RcvXmiewp49e7Q3b95YrDt//rwWNWpUrXHjxhbr27Ztq0WPHl29Lzpbt25V1z5nzhxj3eHDh7Xnz59b7Pvw4UMtceLEWsmSJS3WjxkzRu1/8OBBY92ZM2e0iBEjan369LH7Oh48eKCOM2jQIC0kaN++vTq+Pdy9e1fz8fHRWrVq5W/bx48ftXv37jl8/pC+vvCKxwqFNm3aqAeOH6g9vHv3Ths6dKiWIUMGLUqUKFratGnVD+j169cW5bC+Ro0a2t9//60VLlxY/dDTp0+vLV682CiDlwznNi/YT29M9b/N6PuY2bJli/rBx40bVzXAWbJksfhRX7lyxWbDuW3bNq1UqVJajBgx1L6ff/65dvr0aZvng3BEnVAuTpw4WrNmzexqYHWhsGjRInUPfH19jW1ojHDsX3/91Z9QePTokdatWzctV65cav/YsWNr1apV0/755x+jzI4dO/zdP/N1li1bVsuZM6dqKEuXLq0a1U6dOhnbsOg0adJE1c/6+iHM4sWLp926dUsLLQoUKKAWM0mSJNHq16/vryyedcWKFYM8Zt26dbUECRJYrMN7icUaXHPGjBmdbjQhYCDw48ePr+5twYIFtd9++82izNu3b7XBgwdrmTJlUmVQR7zLeKf198fWMw6Iffv2qe143+wB7yPeiVSpUqnfM6579OjR2ocPHyx+O9YLBYTzeKxPASYNqJQlSpSwq/x3330nAwcOVCq+rmqPGjVKGjZs6K/sxYsX5csvv5TKlSvLhAkTJH78+MrGDnMUgKqNY4BGjRopf8LkyZMdqj+OVbNmTXnz5o0MHTpUnefzzz9XZonA+PPPP6Vq1apy//59pZZ37dpV9u7dq/wocExbA7PP8+fP1bXibzitdZXaHnCtsDOvXr3aWLd8+XLJli2bupfWwK4NhzuubeLEicqMB1MJ7jdMRiB79uzqmkHr1q3V/cMCM4vOo0ePpHr16sq0hHsLc4kt4DtKnDix8u98+PBBrYPZBWamadOmSYoUKSQ0QAfq3r17kihRIgu7P56TLV9TkSJF5NixY0EeF6Yp8zFhdz9x4kSAx4SPDc87uOC9hJkGfpDevXur9xJ+kzp16siaNWuMcnj38B7hucB0BjMqfCtHjx5V2+Hnw+8H6M8XS2CmUvDLL78os09gYDvep59++kmZjadOnare/z59+qjfA8A7MWvWLPU3zGP6+c1mMhJMNA/k6dOnSurXrl3brvLopaL8d999Z7G+e/fuav327duNdejlY92uXbuMdffv31e9IfSAdfSeiLXpxF5NYdKkSeo7emsBYUtTyJcvn+p9okeuc/z4cS1ChAiq12x9vhYtWlgc84svvtASJkwY4DnN14GePvjyyy+NXi16YsmSJdOGDBli8x5A89J7a+brwP2DpmaP+QiaALbNnj3b5jazpgA2b96syg8fPtwwK9oyeYUkS5cuVXWYP3++v2tcsmSJv/I9evRQ26w1VTN4B2FSGTBggL/evfle6syYMUNtO3v2bLA1BTzn3LlzW9QL5psSJUook6FO3rx5lUbtKvMRwPuL8tBQ8J6OHz9eaS3WDBs2TL2bMNmZ6d27tzKhXb9+PcDrI16qKTx79kx9IjrBXocX0HsROt26dTMc1mZy5MghpUuXNr6j15E1a9YgozscAVEj4LffflO9P3u4c+eOciRCa0F0hg6cneiV6ddppk2bNhbfcV3ohev30B6+/vprFcGBXuv27dvVJ9bZAk7/CBE+vTboueNciLrB/dN7kfaA4zRv3tyusggLRs8U2gd6gnCAQ1sILRA51L59eylevLjSWHT0qC1cizWoo7mMNdAwcI/Tp0+vopJcccygePz4sXq+unb58OFDteAZQjtFxBm0H/39hVaBda5i4cKFSuvANUMr6d69u9IqK1asaJxX1ybwHkOD1+uIBc54vHO7du1yWZ2IfzxSKCAED9irJl+7dk01VObIC5AsWTL1cmO7GajB1uAF9PX1FVfRoEEDpfLCrIVoD5ixfv7550AFhF5PNLDW4MeDH8aLFy8CvRZcB3DkWj777DMlgFeuXKmijgoXLuzvXuqg/jCtZc6cWTVcMH1AqMLkgWgZe0mZMqURmWMPiOiBoITQhDkhSZIkQe6DsFAIOOvFOlw0MFAeoaOILlq1apVEjBjR2BY9enT1CROhNXrUll7GDJ4hzG94v9FpgFB19pj2ALMpzGADBgxQz8y8DBo0yBBWAAL4yZMnKmQ0d+7cykyIZ+wM+I1CuB45ckS9y7h2mBAhqMxmXggiRGRZ11GP0NLrSMJRSCqEAmzFJ0+edGg/2MbtwfzDNmPPzKQBnUO3d+vgh4sezY4dO5SmgpccjW6FChWUPTygOjiKM9eig8YdPfDFixcrbSmwEEOEXaJRadGihQwbNkw11Pixd+7c2W6NKDgNG+zzemMAHwZ8PUEB4WbdIdDt27b8M9ZAyKHRQuP4999/+/NfYKyBruFZg3W4N9Y9foRU416jgd28ebNFeCvQ9wnomCC4fhT9+aCHDs3AFnpnAP4f+C/QcON9RUgtOgOzZ89WHR1nQVgtfGxYEE76119/qWeFZ4N6QjM2a1BmIKhIOBMKAD0pjEHAWAGo7YGhv0joYaBHrQPHIH7QupPLFaAnjmNaY6vxQWMJ1RgLnLJoUOGwg6Awx6WbrwMgdtuWCQO9cjgFQwKYMhYsWKDqbMs5r4PeMpyPiLk3g3tidpjaK6DtAT1rmJpg9kPgwdixY5VzEY1+YEDrsWVqsUcgoVdeq1YtNWANzn+c25a2gx6srQFoGKNgPT4D7ygcp9u2bVNaI5yp1uD+o2du65gHDhxQwRf2mlWtwb4A4x5svX/WQEDhvmPBmAcICnQYdKHgqmcMpzqEAoQefgMZM2ZU5wuqjq58x4iHm48AegloAPEConG3Br0YRKbo5g9gHSGEhhhA/XcVeGHRgzSr0niZzZEbuv3WGr2RsGUa0HueKIMeu1nwQGNCb02/zpAADT16/rD5wuwWmGZirYXABmy2CQNdeNkSoI7Sq1cvuX79uroveKYYEAfbfkD3UQfmOzQs1gvWBwa0Ppj/0CHBtQXWKcFgtPXr18uNGzeMdWj0IUww+M3MDz/8oLTFmTNnBholg8g4jKo2CwZ0FGBmsT6mI8Dkhl45/DG2NBGzWQ1+BjMwcUGLMN9zR54xzHCnT5/2tx6aE+6X2fwLnwfuPTQpa3Cu9+/fq79jxIhh9/mJF2gKaHwRGokfJ3r/5hHNCNHEjxUOWZA3b17VSECzwAuCHhh6amhEEGoXULhjcEAvGo0UeqoYoYnwOYTGQaU1O1phk4X5CAIJvR+YPtAYpEqVSkqVKhXg8ceNG6dMFmiIWrZsqXq6CL2ETduRlAKOgh9l//797dLgcG3oPaLXDlMOeuR6L9T8/ODPgbkBPVs0IEhvACejI6AhxH2DzVsPkYXDEo0bzFjQGlwNAhTWrVunNAUId4RGmvnmm2+MvzHiGO8i3rFOnTqpHi6eIXr7Zkc6Oiy4DjxXNGbWx8T7pDeyGPE8d+5c9e7A1IOePYQhfFN68ERwwYhivH+oX6tWrdRzQ6cLjTBGZx8/flyVg2aEe1ywYEGlMUBAQUs0j+7HNoDfAcxR6DAEpGXi2AiphfkUmjM6HvhNrFixQp0T5kdd04T/Avcf7xp+4zgPtEW8a6gDTH8oC40P9YSgxe8P9UQbYW2SIw6ieTgIS8MoyHTp0qlBLBgshUE006ZNswirw+A1hFFiIFrkyJG11KlTBzp4LahQyIBCUgEG8GDwFuqTNWtW7aeffvIXkooBaAipTZEihSqHz0aNGlmE2QU0eO3PP/9U14hBXRiQVqtWrQAHr1mHvOJYWI9j2xuSGhABhaQidDd58uSqfqgnBibZCiXFgKgcOXJokSJFsjl4zRbm4zx79kw9LwwYw/M106VLFxWmi3O7Gj1kNqDFmpMnT6qBZRhsiAF1GPWMEbxmAhrspS/Wz+vGjRsqVBjPHyG4NWvWVAMVHSGgkM1Lly6p8FCEHuO3kjJlSnX8VatWGWUQ/lukSBF1PXjOyCwwYsQINahN5/3799oPP/ygRmUjtDaw5gTPcsqUKVrVqlXVgDScF7/l4sWLa3PnzlVhsWYw+hu/Xwyew+8nUaJEKmwWYazmOuzdu1cNvkMZhqe6Bh/856ggIYQQ4p14rE+BEEJI6EOhQAghxIBCgRBCiAGFAiGEEAMKBUIIIQYUCoQQQgwoFNwIBl5h3gJ7cgZhEI85cZq7CYn6YKSyPiAxMDBnBFIc2JO/iIgaSYypLx3NJUbCJxQKbgKprTEXMEZH66moORG5Z4MUD3heSEiH0bQYob1161a790cqEKRwwEhvJH3EXMW20rVD4NlaRo8eHaxjYtQvRkdjEipCwmyaC28HyeeQw8Wc7ZMTkXs20GKQZgEpGZA6HBoL8lEhwWFgqUsA0l8gFQbyZiE1BlJX6DMEIh04soaaQZZQpHYxkz9//mAfE/NuoK7IGYYUJIQEiItGRhMHyZMnj/bNN99YrAtsJil70lIEBFJEWE9E7yzO1CcgkNICxw0Ke1N5uJIDBw74S/nx6tUrNXcwUjUExZgxY9T+mP9aB7OOYSYx87zdAOUwq5krj4nUEJjxzDzLGyG2oPnIDVy5ckVlWTWnBoZ9HGmYAbQF3WRgnQQP5gIk+YM9H+WRMM08lwOOg/0wKQ2SsKFXiPz8eoZKpOBGFk4kD8NMXkhbjORjZt69e6fqgN4wyqDHiZ6wLVNJUPUBSGaGRG6pU6dWdcEkQqifPRlWMPsXkqjBXINkgsOHD3do3gZXoU+wgzmndXBvkLQQyeTMWVID2h+pvs3pvuFPQnI4pNG2BZIh6hPrOHtMaBHQPjE/AiGBQfORG0CWV6Bn/TRPRN62bVuVMVNPrYypOHXQ2CIbJWzZaFSR5x8Tr6Phx35mkEkUDQoaMTTEEAJoYJE2GvMAYNJ2ZOVE44FG/ddff1XnBRBEo0aNUmnLkdkS/g9kyUQWWH2ydnvrg4YfE6nAxIIGFKnBkRIZmTAhUGDuCCzdMswjMLPp9UUmXHsn6IEPwN7Z+8xzQQQ0yQ8yceqzAurg/gCYayD0bAEhhk4AJiayBvsjLTrqaZ4nAaYpZFXF/UOWYGSwNU+RGpxjItsohAKep/V1EGJgU38gIUr//v2V2o9MkI6Yj2xN6J4/f36VJdI6symya96/f9+irCsnbbe3PmvXrlXlkHXTDDKAIrPmxYsXAzQfde7cWe0L040Orilu3Lh2mY90M5M9S1Agq2uFChX8rT916pTaf/bs2QHuqz9X63sFZsyYobadPXvWWIfnMXnyZJVldtasWSojL8rMnDkz2McEy5cv93c/CbGGmoIbwAQmCBEMTkgnHIZmMMH50qVLbU7+opujzJO2Yy4E9CDNPWj09jFfAXru0CLMk7bDhORMfTZs2KDMLsi5bwbmJJg/Nm7caJGj3wz2LVasmNEbB7imxo0bq150UOC6HIkOCgyYcqyn1tRNSPr2wPYF9u6/Z88eizLQBtDLhzMZzm5oSo4e0zx/N+ZHJiQgKBTCEPixmxt6/Yfu6+vrr6z1ZDbmSdux2AKTnkAoQHAgtBHmEkxYUq1aNfn2228tTFn21gfTlCKE03oKSX3aVFvTmJr3hWnKGvgk7AEz2enzKDsLGmJbM73pNv/ATFr6tuDuHyVKFCU4IYAx6T38O8E5pu7D4TSWJDAoFNwAHLewk1vbfIMCPW57sW4QQmLSdkfq4w7QU0a4pj0ENgUpgHCxnnIU6NNaQvAFBPw56NHbmgLTnv2B7q/Qp3kNzjF1YR2U/4SEbygU3AAiRPQoJHPvOyR7cK6etN1eMBUpHNDWAhBRUPr2wPaFCcsazFdsD5im0TwlZmAEFQkFBzmc5dZO2gMHDhjbAwKDEzH9pXnOZfP+eDZBdQ70AWm6ZhacY+J9w37QAAkJCIakugF9InjrH3RITkTu6knb7QUDphClNH36dIv10DwgBDEfdWD77t+/X823ba4n5oS2B92nYM8SFAjjxXUg+kkH9wNRXjBxmSOPrl+/bgg98/6HDh2yeOYQbvDz1K9f3+L6rIFARXgxevj6vMiOHFMHpqecOXOq+b4JCQhqCm4AvTjY6tGDNocUhvRE5K6ctN1eatWqpcJK+/Xrp8ZQ5M2bV5mkYJrCyODARtf27NlTOa3h0+jUqZMRkgoNAuGYoelTQMOPhrZPnz7K9wIhuXjxYnVN8+fPtyiLkch//fWXhfbRrl07mTt3rko3ARMeNLaJEydK0qRJldPd/IzWrl2r7luaNGmUAMfodwga3Av4Fxw9pj72BHXCPoQEir94JBIqTJw4UU3I/vLlS4v1AU1EHtAIYmw3P0Y9JNU88jYkJm23tz4AobddunTRUqRIoc6J8FfUz3qydlsjmk+cOKGVLVtWixYtmqrrsGHDtPnz54f6iGZ9BHP37t3VvYsaNapWuHBhbdOmTf7Kob62flo3btxQobgIF8azx32/cOGCRZktW7ZolStXNp4PnkGVKlW0bdu22ayTPccEGzduVHWytY0QMz74L3CxQUICOEDRU0emVAzqIiQkwQBFmOvWrFnj7qoQD4dCwY0gSyps0khBoWdKJcTVnDlzRpkMMeraVaZI4r1QKBBCCDFg95QQQogBhQIhhBADCgVCCCEGFAqEEEIMKBQIIYR494jm6PkdH3lLwg6+hyxTZhDvIlqkkP3tvzrG9yfcCQVCSDjFh8YPZ6FQIIR4DxE8O517WIBCgRDiPXACIaehUCCEeA/UFJyGQoEQ4j3Qp+A0FAqEEO+B5iOnoVAghHgPNB85DYUCIcR7oPnIaSgUCCHeAzUFp6FQIIR4D9QUnIZCgRDiPUSgo9lZKBQIId4DzUdOQ6FACPEeaD5yGgoFQoj3QE3BaSgUCCHeAwevOQ2FAiHEe6D5yGkoFAgh3gPNR05DoUAI8R5oPnIaCgVCiPcQgU2as/AOEkK8B2oKTkOhQAjxHuhodhoKBUKI90BHs9NQKBBCvAeaj5yGQoEQ4jX4UCg4DYUCIcRr8GGWVKehUCCEeA3UFJyHQoEQ4jVEiMDoI2ehUCCEeA3UFJyHQoEQ4j1QJjgNhQIhxGug+ch5KBQIIV4DzUfO4xFiNUOGDPLo0SN/6588eaK2EUKIvSGpQS0kDGgKV69elQ8fPvhb/+bNG7l165Zb6kQICXtQUwjjQmHdunXG35s3b5a4ceMa3yEktm3bJunSpXNT7QghYQ0KhTAuFOrUqWM8yKZNm1psixw5shIIEyZMcFPtCCFhDZqHwrhQ+Pjxo/pMnz69HDp0SBIlSuTO6hBCwjjUFLzE0XzlyhUKBEKIS0JSg1ocYdeuXVKrVi1JkSKFEjhr16612N6sWTO13rxUq1bNoszjx4+lcePGEidOHIkXL560bNlS/Pz8LMqcOHFCSpcuLdGiRZPUqVPL2LFj/dXll19+kWzZsqkyuXPnlg0bNlhs1zRNBg4cKMmTJ5fo0aNLpUqV5MKFCxJmNIWpU6dK69at1QXi78Do2LFjqNWLEBJ2cbWm8OLFC8mbN6+0aNFC6tata7MMhMDChQuN71GjRrXYDoFw584d2bp1q7x7906aN2+u2r7ly5er7c+ePZMqVaqoRnz27Nny77//qvNBgKAc2Lt3rzRq1EhGjRolNWvWVPvC/H706FHJlSuXKgNBgrZ08eLFyvoyYMAAqVq1qpw+fVq1s/bio0G8uAFU+vDhw5IwYUL1d2AP+fLlyw4dO3r+Di6oIfFUfA9Nd3cVSAgSzYmuaoo2q4Msc3u27cY9KHx8fGTNmjWGL1TXFBA6b61B6Jw5c0Zy5MihzOOFChVS6zZt2iSfffaZ3Lx5U2kgs2bNkn79+sndu3clSpQoqkzv3r3VMc+ePau+N2jQQAmo9evXG8cuVqyY5MuXTwkSNOM4Vrdu3aR79+5q+9OnTyVp0qSyaNEiadiwoedrCjAZ2frb2yhZIKN0aVJJCuRII8kTx5Wvuvwov+88YWxPkiC2DO9UWyoVzy5xY0WX3UcvStexv8il6w/U9vhxYsiAtjWkYrFskjpZfHno66f2HzJzvTzze20cp1yRLDKoXU3JmSmFvHj1Vpb9fkAGzfhdPnz45LfJnDaJTOvXULJlSKbOc+fBU1m58bCM+HGDvH//qcw3tYrK3KHfWtT/9Zt3Er9Yl1C6W+GP+XN/lKmTJ0jjb5pIzz791LqhgwfKgf175cH9+xIjRgzJmy+/dO7aXdJnyOhv/ydPfKV+3dpy/949+XvfIWWiCM/YYx5CqDsWM+jdW/fw7WXnzp2SJEkSiR8/vlSoUEGGDx+uOrtg3759qsevCwQAjQD1PHDggHzxxReqTJkyZQyBANDDHzNmjPj6+qrjokzXrl0tzosyujBCGwqhgmPrIJqzaNGiat8wIRTCCzGjR5V/z9+SJb/tk5UTP6mCZn6e1Frevf8g9TvPkWcvXkvHbyrIhtk/SP66w+Xl67dKkGDpM2mNnLl8V9IkT6Aad6z7usd8dYzcWVLK2mltZcz8zdJywBJJkSSeTOvbUCJGjKD2AzjHsvUH5Z+zN+Tp85eSO0sqmTGgkUSI4CODpv9u1Ofp81eS94uhxnf36JHhg5P/npBVv/xPsmTJarE+R46cUqNmLUmWPLk8e/pUZs2YJm1atZQNW7ZJxIiW000OHtBP7Q+hQOwzH8EEM2TIEIt1gwYNksGDBzt8PpiOYFaCtePSpUvSt29fqV69umqI8azQUENgmIkUKZIkSJBAbQP4tLaWoIevb4NQwKe+zlzGfAzzfrbKhCmhYC0BzQ8YtrBMmTJJ7dq11Y0Ma2zZc1ottsiUJokUzZNeCtQbrhp80HHkSrn650j5qnpBWbRmn5y+dEcadZ9n7HPl5kMZPP13WTCiiWr0oQl8WaWAnLxwW0b9uEmVuXzjofSbslZ+GtNCRszZIH4v38jVW4/UonP9jq+UKZRZSua37H1qosm9R89D6G4QnZcvXkifXj1k0JDhMnfOLIttX37VwPg7ZcpU0qFjZ6UN3L51S1KnSWNs+/l/y+X58+fSuk072f33rlCtf1gOSe3Tp4+/Nie4WkJDUw8czt88efJIxowZlfZQsWJFCYt4hFA4duyYcphgwFrWrJ96TefPn1eSFt72mTNnKlvZ7t27lX3OW4ga5dPtf/32vbEOtsG3b99LiXwZlVCwRZzY0ZRWoZuGcByYecy8evNOokeLIvmzp5G/j/iPQMiQOpFULpFdftt23GJ9rOhR5dyGoRLBx0eOnb0hg6atMwQWcR0jhw+VMmXKSrHiJfwJBTMvX76U39aslpSpUkmyZMmM9ZcuXpQ5s2bKTyt+lps3b4RSrb1DU3DGVBQUSMuDSMqLFy8qoYBndv/+fYsy79+/VxFJ+vPE5z0rTU//HlQZ83Z9HaKPzGXgdwhzIanQAmALu337thw5ckQtcMJUrlxZedyR6gI2ty5dvMu2fe7qXbl+57EM++FziRc7ukSOFFG6NaskqZLFl2SJ/hvdbSZhvJjSp1V1WfDrXmPd1r1npFjeDPJVtYLKHJQicVzp27q62pY8saWNeceiruK7f5KcWjdY9hy9JENn/WFsu3Dtvnw/ZJkyZTXvv1gJhh2LuknKJPFC7B6ERzZu+EPOnDktHbt0C7DMyhXLpFih/FK8cH7ZvXuXzJm7UCL/v8357du30rtHV+nSvYckT5EiFGvu+ViHh9paQpKbN2+qPG56w1y8eHHliEabprN9+3Y1Rgv2fr0MQl8RmaSDSCV0kGE60ssgw4MZlMF6APMTBIO5DKKa4LfQy4QpoTBu3DgZNmyYhZMMThLY+BBmBWcb4m/NN1YHDiNcvHnRPvrPo+SJwMHbsNtcyZQ2idzZNU4e75soZQplkU27T8lH7ZMWYCZ2zGiyZmpbOXP5jgyf819jvm3/Wek7ea1M7dtQnh6YLCd+Gyibd59S2z5+tHQKfNtrgRT/eow07bNQqpfOKV2a/KfiHjhxRZavPygnzt+S3UcuSsPuc5Vju+WXJUP0PoQn7t65I2NHj5BRY8YF2lv9rObnsvLXNbJg8U+SNm066dGts+EcnTJpgqTPmFFq1qodijUPnwnx/Pz85J9//lGL7tDF39evX1fbevToIfv371f529Ago4MLczecwCB79uzK79CqVSs5ePCg7NmzRzp06KDMTogWAl9//bVyMmP8wqlTp2TlypUyZcoUCxNXp06dVNQSMjwgIgltI6I3cSx13T4+0rlzZ+XkRvoghLU2adJEncMcLRVmzEcInYKKZW0aevDggWrkATz46CHZ4zSKmLSwRE5eRMICx87ckGINR0ucWNEkSuRIqhHetaS7HDl93aJcrBhRZd2MdvL85Wtp0HWuETGkM/Wn7WqBA9r32UtJmyKBDOtYW/kgzNy890R9nr18V0VAzOjfSCYv3eZPeACc4/i5G5IxdeIQufbwyOnTp+Txo0fSsP5/YZEwmx45fEj+t2KZHDr2rzKbxo4dWy0QCHny5JVSJYrI9j+3SvUaNeXQgf1y4cJ5KbBls9pfjyovV6qYfNe6jbTrEH7H9bhaEzh8+LCUL1/e+K431EjLg1BSDDrDuABoA2iAMd4AHVyzwF+2bJlqvGFOwm+uXr16FmOz0AHesmWLtG/fXgoWLKjMT+gE62MUQIkSJdTYhP79+ytndubMmVXkkT5GAfTs2VOFrWI/1KdUqVJKkDgyRsFjhAKkKwZrQAoWLlxYrUNcL+JtdSkHKZslSxa7nEZJSveSsIYeXpoxTWIVvoqQU7OG8PvM9vLm7Xv5svMc9RkQCDUFX1UrJDfuPFZ+gYCAqQkmK3zaEgpYjxDXzQE4yonjFC1WTFat/S/aCwzq10fSZcggzVu28hddBNSTUb6mT52iCZOnyes3/4Ujnzr5rwzq31cWLlkmqVL/54gOj+CddSXlypUzhK4tkMgzKBAgow9UCwg4qP/+++9Ay9SvX18tgQnEoUOHqsUZPEIozJkzR/kLoFLBCaOHbUEaT5o0SX2Hw3nevP+icAJzGvlE8P/Dchcxo0ex6GmnS5lQ8mRJqXrzN+76St1K+eWBr5/cuPtYcmVOIeN7fKnGIcAkpAuE9TPbK6dx836LJU7MaGoB2E9vzGEG2rL3jLJV1q6YT7o3ryzf9FxgbG9YvZAKSz158bYSKgVzpFG+jFVbjhhaR5/W1eTgiaty6cYD5ePo0rSSCoFduOY//wVxjpgxY0nmzJadm+gxYki8uPHU+ps3bsjmTRukeImSEj9+Arl3764smPejRI0aTUqVKavKmyOQwBNfX/WJcQzhfZwCcx85j0cIhVixYsncuXOVANBHL8OLj/U6jnrQPYUCOdLKlnmdjO9ju9dTn0vX7ZfWg36SZInjyJhudSVJwthy9+EzWbb+gBFaCvJlSy1F8nyKYT79u2UcddbPBipHNahSMof0/K6qRI0cSY2LqN/lR4tQ2PcfPkrXZpXVIDb8cLDfrJW7ZNpP240y8WPHkJkDv5akCWOL77NXcuzMdSnfbKIyNZHQIUrUKHL0yGH5aeliefb0mSRMlFAKFiwkS5atMAZEkYChTHAet6W5sAXCuDAABJFGSOiEqgVH8jPNhXfDNBfejTNpLrL1Dtqcc3b0Jycw8eDoI4RwwQkDnwFygiB5FIA3HuMTCCHEXp9CUAsJA0IB/gRMqoMwL4Sf6iAJFLznhBBiDxQKXuJTQDgWvPipUqWyWI+wq2vXrrmtXoSQsAV9Cl4iFBBba9YQdDAUPKSGoxNCvA9GH3mJ+QgzDi1ZssTiwSK0EqOZESdMCCH2QPORl2gKaPzhaMboQQzQwcg8DPeGpoBh4YQQYg/UFLxEU8BQbWRFxbBsjG6GOQk5yjGKGRNNEEKIPUAmBLWQMDROwZrjx49LgQIFVG4YR+A4Be+G4xS8G2fGKRQaviPIMof7/5fLiHio+YgQQlwBzUfOQ6FACPEa6Eh2HgoFQojXQEUhjAsFOJMDAznBCSHEXmg+CuNCAZNLBLUdswcRQog90HwUxoXCwoUL3Xl6QoiXQUXBeehTIIR4DZjukjgHhQIhxGugpuA8FAqEEK+BjmbnoVAghHgNdDQ7D4UCIcRroKLgPBQKhBCvISI1BaehUCCEeA30KTgPhQIhxGugohBKQmHdunV2H/Dzzz93pj6EEBJs6GgOJaFQp04du1U3R+c+IIQQV+EjFAqhIhQwXzIhhHg6dDQ7D30KhBCvgX5mNwkFzKH8119/yfXr1+Xt27cW2zp27OiCahFCiONEoFQIfaFw7Ngx+eyzz+Tly5dKOCRIkEAePnwoMWLEkCRJklAoEELcBh3NzuNwSsEuXbpIrVq1xNfXV6JHjy779++Xa9euScGCBWX8+PEuqBIhhAQPKApBLcTFQuGff/6Rbt26qRS1ESNGlDdv3kjq1Kll7Nix0rdvX0cPRwghLiOij0+QC3GxUIgcObKRsxzmIvgV9FnSbty44ejhCCHEZSAsPqiFuNinkD9/fjl06JBkzpxZypYtKwMHDlQ+haVLl0quXLkcPRwhhLgMuhTcoCmMHDlSkidPrv4eMWKExI8fX9q2bSsPHjyQH3/80QVVIoSQ4Duag1qIi4VCoUKFpHz58ob5aNOmTfLs2TM5cuSI5M2b19HDEUKIx5qPdu3apQJrUqRIofZdu3atxXZN05S1BB1lBN5UqlRJLly4YFHm8ePH0rhxY4kTJ47EixdPWrZsKX5+fhZlTpw4IaVLl5Zo0aIZPlprfvnlF8mWLZsqkzt3btmwYYPDdbEHTmhKCPGqEc1BLY7w4sUL1dmdMWOGze1ovKdOnSqzZ8+WAwcOSMyYMaVq1ary+vVrowwEwqlTp2Tr1q2yfv16JWhat25tbEenukqVKpI2bVrVuR43bpwMHjzYwvKyd+9eadSokRIoGBaA1ENYTp486VBd7MFHg3hxgPTp0wcqbS9fvizuJnr+Du6uAglBfA9Nd3cVSAgSzYk8Cy3+92+QZRY0zB2sY/v4+MiaNWuMXHBoOqFBIBqze/fuat3Tp08ladKksmjRImnYsKGcOXNGcuTIofywsLIAWFcw1uvmzZtq/1mzZkm/fv3k7t27EiVKFFWmd+/eSis5e/as+t6gQQMloCBUdIoVKyb58uVTQsCeuoSYptC5c2fp1KmTsbRr106KFy+uKmCWfoQQ4o4RzUEtruLKlSuqIYeZRgdRmEWLFpV9+/ap7/iEyUgXCADlEcGJ3rxepkyZMoZAAOjhnzt3To0H08uYz6OX0c9jT13sxWGZDEFgC6hXhw8fdvRwhBDiMuxxJGNsFRYzUaNGVYsjoBEG6I2bwXd9Gz7hezUTKVIklQnCXAYWGOtj6NsQzIPPoM4TVF1C3adQvXp1+fXXX111OEIICZERzaNGjVK9aPOCdcTFQmHVqlVK+hFCiCc7mvv06aPM3eYF6xwlWbJk6vPevXsW6/Fd34bP+/fvW2x///69ikgyl7F1DPM5Aipj3h5UXUJMKGDwWoECBYwF3xEChRQXTHNBCPH0kFSYiRAeal4cNR0BmHzQ4G7btk3MkUTwFcDPCvD55MkTFVWks337djVHDez9ehlEJL17984og0ilrFmzKtORXsZ8Hr2Mfh576hJiPoXatWtbRB/BYZI4cWIpV66ciqH1BBidQkj4xNUx9n5+fnLx4kXjOxy6yP8Gq0iaNGlU4M3w4cNVhgc0zAMGDFBRQHqEUvbs2aVatWrSqlUrFSWEhr9Dhw4qGgjlwNdffy1DhgxR4aa9evVSYaZTpkyRSZMmWfhykUFiwoQJUqNGDfnf//6nfLh62Cra5KDqEmIhqWGB1+/dXQNCiDtCUjv/9imEMzAm17a/87pz505jsK6Zpk2bqlBPNJ+DBg1SjTM0glKlSsnMmTMlS5YsRlmYiiAIfv/9d9WJrlevnhpPECtWLIvBa+3bt1ehq4kSJZIffvhBCQjrwWv9+/eXq1evqoYf4xIQ2qpjT11CRCggM+qdO3f8edQfPXqk1nnCHM0UCoSET6HQdV3QQmHi555h0fBUHL79AckQhHiZ42wJISS0YRbUUBQKUHf0mz5v3jwL1QfaARwlnuJTIISETyIycU/oCQXd6QFNAQ4TmJF0oCGkS5dOrSeEEHfBOZpDUSjA6w7gdFm9erURKkUIIZ5CRMqE0Pcp7Nixw/mzEkJICEBNwXkctsAhnGrMmDH+1iM8qn79+i6oEiGEhFyaC+JioQCHsjk21pz7CNsIIcRdRIrgE+RCXGw+wgg/W6GnkSNHVsOqCSHEXVATcIOmgGngVq5c6W89hl1jMglCCHEXEX18glyIizUF5NOoW7euXLp0SSpUqKDWIQnT8uXLVaZUQghxF7QOuUEoYBJrTBM3cuRIJQQwQTTmMEXmP6bOJoS4EwoF53E6IR78CCtWrJD58+er9LDMfUQIcVfuowl/BT1HfLeyGYJ/gnBAsAeFI9IImQKRmhXpXGFK2r9/v2trRwghDsCQVOdxSCZjrk+ki4VWAA3hq6++UonwYE6ik5kQ4m4YchqKmgJ8CZgJCHm/J0+eLLdv35Zp06a5oAqEEOIaqCmEoqawceNG6dixo7Rt21ZN8EAIIZ5GBGGrH2qawu7du+X58+dSsGBBNbfo9OnT5eHDh05XgBBCXJk6O6iFBI7dt6hYsWIyd+5cNeva999/rwarwcmMCagxgTQEBiGEuDshXlALCcGQ1HPnzimn89KlS9WcoJUrV5Z169aJu2FIKiHhMyR1/sHrQZZpWSRN8E8QDnBKmYLjGdlRb968qcYqEEKIO6Gj2QMGr3ki1BQICZ+awqJDQWsKzQpTUwgMJ24/IYR4FvQZOA+FAiHEa6BQcB4KBUKI18ABzc5DoUAI8Rp8qCk4DYUCIcRr4Ng056FQIIR4DfQphGGhgNnb7GX16tUhWhdCiHdA81EYFgpx48Y1/sZQiTVr1qh1hQoVUuswYQ9GSTsiPAgh4RvOwRyGhcLChQuNv3v16qXmZpg9e7ZEjBhRrcMMbu3atZM4ceK4q4qEkDAGRYKXjGhOnDixysKKtBnWuZVKlCghjx49cuh4HNFMSPgc0fzbv3eDLFM7d7LgnyAc4BHO+vfv38vZs2f9rcc6ZGElhBB7zUdBLSQMRB81b95cWrZsKZcuXZIiRYqodQcOHJDRo0erbYQQYg8+NCB5h1AYP368JEuWTCZMmKDmawDJkyeXHj16SLdu3dxdPUJIGIGKgJf4FMw8e/ZMfTrjYKZPgZDw6VPYfPpBkGWq5kgc/BOEAzxCUzDDaCNCSHChpuAlQiF9+vSBDjq5fPlyqNaHEBI2oSPZS6KPOnfuLJ06dTIWjE8oXry4PH36VFq3bi3hjflz58jXX9WT4oXzS7nSxaXzD+3k6pX/BOPTJ09k1Ihh8nmNqlKkQB6pWrGcjB453OY82b+tWS1fflFLCufPrY41ctiQUL4aYosXL/xk7KgRUq1SefUMmzRuKCf/PWFsh1V3xrQpUrFsKbW9dctmcu3aVWP7oYMHJG/OrDYX83HCo6M5qH+OMHjwYNVhNS/ZsmUztr9+/Vrat28vCRMmlFixYkm9evXk3r17Fse4fv261KhRQ2LEiCFJkiRRvlJEXJrZuXOnFChQQKJGjSqZMmWSRYsW+avLjBkzJF26dBItWjQpWrSoHDx4ULxWU4AgsAVuwuHDhyW8cfjQQWnQqLHkzJ1bPrz/INOmTJQ2rVrK6nV/qBfr/oP78uD+fenavZdkzJhJbt++JcOHDlbrJkyeahxnyaKFsmTxAunarafkzpNXXr16Kbdv3XLrtZFPDB7YXy5euCAjRo+VxImTyB/r18n33zWX1es2SNKkSWXh/LmyYtlSGTZytKRMmUoJiLatW8qadRtUw5EvX37ZtnO3xTFR5sCBfZIzV24Jr4SEopAzZ075888/je+RIv3XbHbp0kX++OMP+eWXX1RGhg4dOqgsDHv27DEG4UIgIJBm7969KpCmSZMmEjlyZBk5cqQqc+XKFVWmTZs2smzZMtm2bZt89913KtimatWqqszKlSula9euaoAvBMLkyZPVNozlgqBxKZoHc+nSJS127NgO7/fqnXctt+490rJkyaLt3ncwwDK/rd+g5cyZU3v+6p36fu/hEy1Pnjzazr/3ur3+XCwX3+evtOzZs2ub/9xhsb52nS+0ceMnai/fftRKlCipzf5xnrHt/uNnWq5cubTVv623ecxnL99qRYsV0yZPne7263N2cYa/zj0KcnGEQYMGaXnz5rW57cmTJ1rkyJG1X375xVh35swZBO5o+/btU983bNigRYgQQbt7965RZtasWVqcOHG0N2/eqO89e/ZUv10zDRo00KpWrWp8L1KkiNa+fXvj+4cPH7QUKVJoo0aN0lyNR5iPAmLVqlWSIEECCe/4/b9ZKI4pX5T/Mn5KfdV7Mfv27VED/+7fuyd1alWXyhXKSI+uneTu/4f8Evfx4cN71YNEj98Mvh87dlRu3bwpDx8+kKLFShjbYseOrbS9E8eP2TzmXzu2K7NinS/qSXjGHvPRmzdvVJSjecG6gLhw4YKkSJFCMmTIII0bN1bmID0/27t376RSpUpGWZiW0qRJI/v27VPf8Zk7d26l/emgh49znjp1yihjPoZeRj/G27dv1bnMZSJEiKC+62VciUcIhfz58yt7mr7gO1Snvn37qiU8g4Z97JiRki9/AcmcOYvNMr6+j+XH2TOlXv0GxrqbN27Kx4+azJs7W3r06isTJk1VPprvWzWXd2/fhuIVEGtixowlefPlV8/s/v17SkCs//03OXH8H3nw4L4SCCBhooQW+8Fu/fDhQ5vHXLN6lZQoWUqSJgvfKRww81pQy6hRo5Spx7xgnS2KFi2q7PubNm2SWbNmKVNP6dKllf/u7t27EiVKFIkXL57FPhAA2AbwaRYI+nZ9W2BlIDhevXqlnjneEVtl9GN4nU+hTp06Ft8hBZEPqVy5chZOHVtAwltLeS1iVH+9sLDKyOFD5NKFC7Jo6XKb2/38/KRD2+8lQ8aM0qZdB2O9pn2U9+/fSa8+/VVjAUaPmygVy5aUgwcPSMlSpUPtGoh/RowaK4MG9JXK5cuoJJDZsueQap/VkDOnP/UeHeHe3buyd89uGTdhsoR37JlPoU+fPso+byag9qJ69erG33ny5FFCIm3atPLzzz9L9OjRxRvxCKEwaNCgYO8LCT9kiGVETb8Bg6T/wMES1hk5fKjs+munLFj8k80eICJY2n3/ncSMGVMmTZ2hnFc6iRJ/GqADR7QOTHHx4senCckDSJ0mjXquL1++VM8RzuYe3TpLqlSpJVGiT8/u0cNHar0OEkNmtdFJWrvmV4kbL56ULV9Bwjv2+JkhAILbaYwXL55kyZJFLl68KJUrV1amHaT4N2sLiD6CYxng0zpKSI9OMpexjljCd4zZguBBpwGLrTL6MbzOfGQGIV7W9r6gpD7MIualR68+EpZBOCIEwvZtW2XugsWqobClISAiCYJgyvRZ/l5ymJvA1atXjHWwOT/x9ZXkKVKEwlUQe0A0GRr+Z0+fyr49u6Vc+YqSMlUqJRgQSWR+3v+eOC558ub39678tna11Pq8jkWnILxiHT5qa3EGPz8/laMN5u2CBQuqe45oIR1EA8HngJB6gM9///1X7t+/b5TZunWravBz5MhhlDEfQy+jHwMmKpzLXAZmZXzXy3idpvDixQs1pwJUMltpsmFPc0Tqh/U0FxhLsHHDepk8babEjBFTHj74ZGOOFTu2ilH+JBBayOvXr2Tk6HHyws9PLSB+ggSqV5EuXXopX6GijBk1QgYOHioxY8WSqZMmSrr0GaRwkaJuvkKyZ/ffaNElbfr0cuP6dZk0fqx6NrW/qKsarsbfNpG5c2ZJ2jRplZBAuGniJEmkQkVLh+TBA/uVY7puvS/ddi3eHJLavXt3qVWrljIZ3b59W1k18Ptq1KiR8kUgkSdMUdDC0dD/8MMPqqEuVqyY2r9KlSqq8f/2229l7NixygfQv39/NbZBb7cQijp9+nTp2bOntGjRQrZv367aQoS66uAcTZs2VZOQIWkoQlLRboZEwlCPEAq4GTt27FCOHNw8jE+4deuWzJkzR2VKDW/8vHKF+mzZ7FuL9UOHj1KNBuzO6DWCmtUrW5TZsGWbimsHw0eNlXFjRkqHdt9LBJ8IUrBwYZk1Zx57lB6An99zmTp5ovIHxI0bTypWriI/dOpiPJvmLVspJ+PQwQPl+fNnkr9AQZk5Z56/DtCaX1epMQvpM2R005V4t1C4efOmEgDorMLPWapUKdm/f7/6G0yaNEn5QDFoDb5NRA3NnDnT2B8CZP369dK2bVslLGDqReM+dOhQi4wOEAAY8zBlyhRJlSqVzJs3zxijABo0aCAPHjyQgQMHKsGSL18+5fy2dj57TUI8hHAtWbJEOZYhbY8ePapG9S1dulRWrFghGzZscOh4YV1TICQ840xCvMNXAjc3g0LpmV/N430Kjx8/VjHAAEIB3wGk8q5du9xcO0JIWNIUglpIGBAKEAiI/wUIQYU9Dfz+++/+YoAJISQgKBS8RCjAWXL8+Ccbee/evZVPAQ5V2NiQPIoQQtyREC884hE+BWuuXbumhnXDr4ABI45CnwIh4dOncPy6/0zB1uRNEzv4JwgHuF1TQO6QihUrqvwiOgj/QqbB4AgEQkg4xseOhXh2SCpC8E6cCL/53wkhroPmIS/QFMA333wj8+fPd3c1CCHhICEe8XBNAWAWogULFqiJLDCcGwM8zEycONFtdSOEhCHY6IdtoYC5lzG93MmTJ1XKbHD+/HmLMs7mKiGEhB/syZJKPDj6CEPAMT2dPp0chnJPnTrV6aHbjD4iJHxGH525/SLIMtlTWFoiiAdpCtbyaOPGjSrJEyGEBAsqCt7hU9DxwCEThJAwBM1HYVwo2MpvTh8CISS4sPXwAvNRs2bNjHTAmGAHucWto49Wr17tphoSQsIS7FSGcaGAvOLW4xUIISS4UCZ4ae4jZ2H0ESHhM/ro0v1XQZbJmCR68E8QDvAoRzMhhDgDzUfOQ6FACPEaKBOch0KBEOI1UCg4D4UCIcRrYJZU56FQIIR4DdQUnIdCgRDiNTA1tvNQKBBCvAhKBWehUCCEeA00HzkPhQIhxGug+ch5KBQIIV4Do4+ch0KBEOI10HzkPBQKhBCvgULBeSgUCCFeA81HzkOhQAjxGqgpOA+FAiHEa6BQcB4KBUKI18A5mp0ngguOQQghxEugpkAI8RqoKDgPhQIhxGug+ch5KBQIIV4DRYLzUCgQQrwGztHsPBQKhBCvgTLBeSgUCCFeA2WC81AoEEK8BpqPnIdCgRDiNVAmOI+PpmmaC45D3MSbN29k1KhR0qdPH4kaNaq7q0NcDJ8vCW0oFMI4z549k7hx48rTp08lTpw47q4OcTF8viS0YZoLQgghBhQKhBBCDCgUCCGEGFAohHHgfBw0aBCdkF4Kny8JbehoJoQQYkBNgRBCiAGFAiGEEAMKhXBAunTpZPLkye6uBnETzZo1kzp16ri7GiSMQKEQDn60hw4dktatW1vkh1m7dm2InY/891xxr62XixcvurtqhAQIcx+FAxInTuzuKoRbqlWrJgsXLgz0ebx9+1aiRIkSyjUjxDbUFDyckydPSvXq1SVWrFiSNGlS+fbbb+Xhw4fG9ufPn0vjxo0lZsyYkjx5cpk0aZKUK1dOOnfubNN8hL/BF198oXqt+ncSMiCUNFmyZBZLxYoVpUOHDuoZJUqUSKpWrarKTpw4UXLnzq2eZerUqaVdu3bi5+dnHGvw4MGSL18+i+PjuZqf4YcPH6Rr164SL148SZgwofTs2VMYYEgcgULBg3ny5IlUqFBB8ufPL4cPH5ZNmzbJvXv35KuvvjLKoAHYs2ePrFu3TrZu3Sp///23HD16NFBTEkDv9c6dO8Z3ErosXrxYaQd4drNnz1brIkSIIFOnTpVTp06p7du3b1eNuiNMmDBBFi1aJAsWLJDdu3fL48ePZc2aNSF0FcQbofnIg5k+fboSCCNHjjTW4ceOXuT58+eVZoDGY/ny5ar3qTf2KVKkCPCYuukCPUn0WknIsn79eqXl6UDrA5kzZ5axY8dalLXW7oYPHy5t2rSRmTNn2n0+aA7IqFq3bl31HQJn8+bNLrgSEl6gUPBgjh8/Ljt27LBoVHQuXbokr169knfv3kmRIkWM9ciomTVr1lCuKQmI8uXLy6xZs4zvMA01atRIChYs6K/sn3/+qdJknz17VmVHff/+vbx+/VpevnwpMWLECPJcyKQK7a9o0aLGukiRIkmhQoVoQiJ2Q6HgwcCeXKtWLRkzZoy/bdASGMXi+UAIZMqUyeZ6M1evXpWaNWtK27ZtZcSIEZIgQQJl/mnZsqVyREMowLxk3bijU0CIK6FPwYMpUKCAsi/DlICGxbygUcmQIYNEjhzZwi+A3iJMS4GBfeCQJJ7DkSNH5OPHj8onUKxYMcmSJYvcvn3bn+nv7t27FoLhn3/+sdAS0Vk4cOCAsQ7aBo5NiL1QKHgIaMzxAzcvGFsARyHMDWj4YTKCfbh58+aqUY8dO7Y0bdpUevToocxMECDoWaJHGdhctRAy27ZtUw2Mr69vqF4nsQ0EPXr906ZNk8uXL8vSpUsNB7QOosoePHigfBF4F2bMmCEbN260KNOpUycZPXq0GocCMxQimBCwQIi9UCh4CDt37lROZfMybNgwFZ0CAVClShUVrghnJJzEaPj1MMbixYsr00OlSpWkZMmSkj17dokWLVqA50JvFJFKcFjjPMT95M2bVz1LmApz5coly5YtU/4FM3iucDpDGKD8wYMHpXv37hZlunXrpsKW0VnAe4GOA8KPCbEXZkn1Ml68eCEpU6ZUDT+0BkIIcQQ6msM4x44dU2YCRCDBBDV06FC1vnbt2u6uGiEkDEKh4AWMHz9ezp07pwZDIdQRA9gwUpYQQhyF5iNCCCEGdDQTQggxoFAghBBiQKFACCHEgEKBEEKIAYUCIYQQAwoFEqamKrWeQCg0R5wjdQhTRhBvh0KBOD3/MMZHIHcPBs4hAVtIsnr1apX+wx7YkBPiOBy8Rpyef/jNmzeyYcMGad++vcrAikleQmoOYqSUJoSEHNQUiNPzD6dNm1bNA4CEfJgWVDf5YF4AzAKnT/pz48YNNZUoEvqhcUcqDswj4Mj8wtbmIwikXr16qeR+qA80lvnz56vjYoIbED9+fKUxoF4AKaqRbC59+vQSPXp0lVxu1apVFueBkEP6amzHccz1JMSboVAgLgMNKLQCgNTcSL2BbKyYkhJpoTFBPbJ2Ig0Hsr9iRjloG/o+wZlfuEmTJrJixQo1t/GZM2dkzpw56rgQEr/++qsqg3pgRrIpU6ao7xAIS5YsUampkW68S5cu8s0338hff/1lCC9MZ4kJjpDC/LvvvpPevXuH8N0jxENAmgtCHKVp06Za7dq11d8fP37Utm7dqkWNGlXr3r272pY0aVLtzZs3RvmlS5dqWbNmVWV1sD169Oja5s2b1ffkyZNrY8eONba/e/dOS5UqlXEeULZsWa1Tp07q73PnzkGNUOe2xY4dO9R2X19fY93r16+1GDFiaHv37rUo27JlS61Ro0bq7z59+mg5cuSw2N6rVy9/xyLEG6FPgTg9KT20AJhkvv76axk8eLDyLWDuB7MfAfNNY/pQaApmMAcxJowJzvzC6MVHjBhRypYta3edUQfMeVy5cmWL9dBW9LkloHGY6wEwNwEh4QEKBeL0pPRo/OE7QCMe0BzEmG8aGVwxeYw1mGYyOMBc5SioB/jjjz/UvBNm4JMgJLxDoUBcPil9QPNNr1y5UpIkSSJx4sSxWUafX7hMmTIW8wtjX1tAG4GGAl8AnNzW6JqKeT7qHDlyqMb/+vXrAWoYmOEMDnMz+/fvt+s6CQnr0NFMQoXGjRurOR4QcQRH85UrV9Q4go4dO8rNmzeDNb8w5prGtJMtWrRQ++jH/Pnnn9V2REUh6ghmLsxtDC0B5itMYQnn8uLFi5Xp6ujRo2puZHwHbdq0kQsXLqi5r+GkXr58uXKAExIeoFAgoUKMGDFk165dkiZNGhXZg944pguFT0HXHIIzvzDMV19++aUSINmyZZNWrVqpKUkBzENDhgxRkUNJkyaVDh06qPUY/DZgwAAVhYR6IAIK5iSEqALUEZFLEDQIV0WU0siRI0P8HhHiCXCSHUIIIQbUFAghhBhQKBBCCDGgUCCEEGJAoUAIIcSAQoEQQogBhQIhhBADCgVCCCEGFAqEEEIMKBQIIYQYUCgQQggxoFAghBBiQKFACCFEdP4PMA46gHnsJtgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative : 192,935  (legit correctly flagged safe)\n",
      "False Positive: 434  (legit incorrectly flagged fraud)\n",
      "False Negative: 226  (fraud missed – costly)\n",
      "True Positive : 907  (fraud correctly caught)\n"
     ]
    }
   ],
   "source": [
    "# Confusion-matrix heat-map \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "# --- ensure output directory exists ------------------------------------\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "# --- use optimal threshold we already found -----------------------------\n",
    "opt_thresh = test_report['threshold']        # from Cell 34\n",
    "y_pred     = (y_proba >= opt_thresh).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = ['Legit', 'Fraud']\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'Confusion Matrix – 2020 Test Set\\n(threshold = {opt_thresh:.3f})')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/cm_v0_4_2020_test.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- print counts for clarity ------------------------------------------\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negative : {tn:,}  (legit correctly flagged safe)\")\n",
    "print(f\"False Positive: {fp:,}  (legit incorrectly flagged fraud)\")\n",
    "print(f\"False Negative: {fn:,}  (fraud missed – costly)\")\n",
    "print(f\"True Positive : {tp:,}  (fraud correctly caught)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
